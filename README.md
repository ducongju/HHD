# Heterogeneous heatmap distillation framework based on unbiased alignment for lightweight human pose estimation

## Introduction
The growing demand for mobile devices has generated interest in lightweight human pose estimation. Currently, lightweight estimation generally uses heatmap-based methods, which has demonstrated exceptional performance. However, their use of non-differentiable post-processing imposes considerable inference latencies. Conversely, integral-based approaches expedite the inference process by employing a soft-argmax operation but compromise in accuracy. Integrating explicit heatmap knowledge learned using the heatmap-based method into the implicit heatmap generated by the integral-based method, thereby combining the best of both worlds, offers a promising avenue. However, owing to the disparities in supervision and inference processes, the explicit and implicit heatmaps are heterogeneous. Consequently, direct transfer of knowledge presents difficulties in ensuring consistencies in heat value and location. In this paper, we propose a novel Heterogeneous Heatmap Distillation (HHD) framework that effectively tackles these challenges, thereby enhancing the performance of the integral-based approach to a level comparable to that of the heatmap-based method. The framework revolves around an unbiased heatmap alignment scheme encompassing two steps: heterogeneous heatmap normalization and unbiased cropping. Heterogeneous heatmap normalization separately normalizes the output feature maps of both the teacher and student models, alleviating potential heat value bias during the knowledge transfer. Unbiased cropping applies closed-form computation on the normalized teacher and student heatmap to eliminate location bias. Additionally, reflection padding is implemented to handle potential cases wherein the cropped region extends beyond the image boundary. Extensive experiments demonstrate the efficiency and effectiveness of our methods on the MSCOCO and MPII datasets compared to other integral-based lightweight networks.

## Environment

The code is developed using python 3.7, torch 1.10, torchvision 0.11 on cuda 11.1. NVIDIA GPUs are needed. The code is developed and tested using a single NVIDIA 2080Ti GPU cards for Lite-HRNet. Other platforms are not fully tested.

## Quick start

### Prepare the directory

1. Clone this repo, and we'll call the directory that you cloned as ${POSE_ROOT}.

2. Init output(training model output directory) and log(tensorboard log directory) directory:

   ```
   mkdir work_dirs
   mkdir test_dirs
   mkdir test_results
   mkdir vis_input
   mkdir vis_output
   mkdir data
   ```

### Data preparation

**For [COCO](http://cocodataset.org/) data**, please download from [COCO download](http://cocodataset.org/#download), 2017 Train/Val is needed for COCO keypoints training and validation. [HRNet-Human-Pose-Estimation](https://github.com/HRNet/HRNet-Human-Pose-Estimation) provides person detection result of COCO val2017 to reproduce our multi-person pose estimation results. Please download from [OneDrive](https://1drv.ms/f/s!AhIXJn_J-blWzzDXoz5BeFl8sWM-) or [GoogleDrive](https://drive.google.com/drive/folders/1fRUDNUDxe9fjqcRZ2bnF_TKMlO0nB_dk?usp=sharing). Optionally, to evaluate on COCO'2017 test-dev, please download the [image-info](https://download.openmmlab.com/mmpose/datasets/person_keypoints_test-dev-2017.json). Download and extract them under {POSE_ROOT}/data, and make them look like this:

    ${POSE_ROOT}
    |-- data
    `-- |-- coco
        `-- │-- annotations
                │   │-- person_keypoints_train2017.json
                │   |-- person_keypoints_val2017.json
                │   |-- person_keypoints_test-dev-2017.json
                |-- person_detection_results
                |   |-- COCO_val2017_detections_AP_H_56_person.json
                |   |-- COCO_test-dev2017_detections_AP_H_609_person.json
                │-- train2017
                │   │-- 000000000009.jpg
                │   │-- 000000000025.jpg
                │   │-- 000000000030.jpg
                │   │-- ...
                `-- val2017
                    │-- 000000000139.jpg
                    │-- 000000000285.jpg
                    │-- 000000000632.jpg
                    │-- ...


**For [MPII](http://human-pose.mpi-inf.mpg.de/) data**, please download from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/). We have converted the original annotation files into json format, please download them from [mpii_annotations](https://download.openmmlab.com/mmpose/datasets/mpii_annotations.tar). Extract them under {POSE_ROOT}/data, and make them look like this:

    ${POSE_ROOT}
    |-- data
    `-- │-- mpii
        `-- |-- annotations
            |   |-- mpii_gt_val.mat
            |   |-- mpii_test.json
            |   |-- mpii_train.json
            |   |-- mpii_trainval.json
            |   `-- mpii_val.json
            `-- images
                |-- 000001163.jpg
                |-- 000003072.jpg
                │-- ...


### Download the pretrained models

Download pretrained models and our well-trained models from [Google Drive](https://drive.google.com/drive/folders/1bKEuo4f3YfaoiPaiSyGCj5LJr0DLYlF8?usp=sharing) and make models directory look like this:

    ${POSE_ROOT}
    |-- work_dir       
    `-- |-- |-- HHD_COCO_Lite18.pth
            |-- HHD_COCO_Lite30.pth
            |-- HHD_MPII_Lite18.pth
            `-- HHD_MPII_Lite30.pth

### Prepare the environment, Training, and Testing 

You can follow the [UserGuide_Pose](https://mmpose.readthedocs.io/en/latest/) and [UserGuide_Razor](https://mmrazor.readthedocs.io/en/latest/)

### Acknowledge
Thanks for the open-source [MMPose](https://github.com/open-mmlab/mmpose) and [MMRazor](https://github.com/open-mmlab/mmrazor), it is a part of the [OpenMMLab](https://github.com/open-mmlab/) project.
### Other implementations
* [HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation](https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation). It is a part of the [HRNet](https://github.com/HRNet) project.
